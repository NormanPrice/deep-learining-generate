{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install np_utils","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport re\nfrom keras.layers import Dense, LSTM, Input, Embedding, Dropout\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import RMSprop\nfrom keras.utils import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import LambdaCallback\n#import np_utils\nfrom keras.models import load_model, save_model\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-11-10T17:30:39.537293Z","iopub.execute_input":"2023-11-10T17:30:39.537721Z","iopub.status.idle":"2023-11-10T17:30:39.543776Z","shell.execute_reply.started":"2023-11-10T17:30:39.537688Z","shell.execute_reply":"2023-11-10T17:30:39.542848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"load_saved_model = False\ntrain_model = True\nseq_length = 20\nfilename_charachters = \"/kaggle/input/ascii-printable/ascii_printable_characters.txt\"\nfilename = \"/kaggle/input/1m-set/passwords.txt\" \n","metadata":{"execution":{"iopub.status.busy":"2023-11-10T17:30:40.815917Z","iopub.execute_input":"2023-11-10T17:30:40.816679Z","iopub.status.idle":"2023-11-10T17:30:40.821262Z","shell.execute_reply.started":"2023-11-10T17:30:40.816647Z","shell.execute_reply":"2023-11-10T17:30:40.820293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(filename_charachters, encoding='utf-8') as f:\n    text_chars = f.read().strip()\n\nchars = sorted(list(set(text_chars)))\ntotal_chars = len(chars)\nprint(\"Unique characters in the text:\", total_chars)\n\nchar_indices = dict((c, i) for i, c in enumerate(chars))\nindices_char = dict((i, c) for i, c in enumerate(chars))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-10T17:30:42.919788Z","iopub.execute_input":"2023-11-10T17:30:42.920156Z","iopub.status.idle":"2023-11-10T17:30:42.929379Z","shell.execute_reply.started":"2023-11-10T17:30:42.920129Z","shell.execute_reply":"2023-11-10T17:30:42.928283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(filename, encoding='utf-8') as f:\n    text = f.read().strip()","metadata":{"execution":{"iopub.status.busy":"2023-11-10T17:30:45.097401Z","iopub.execute_input":"2023-11-10T17:30:45.097790Z","iopub.status.idle":"2023-11-10T17:30:45.201091Z","shell.execute_reply.started":"2023-11-10T17:30:45.097756Z","shell.execute_reply":"2023-11-10T17:30:45.200167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_non_ascii_printable(file_path):\n    \"\"\"\n    Remove non-ASCII printable characters from a text file.\n\n    :param file_path: Path to the text file to be processed.\n    :return: True if the file was processed successfully, False otherwise.\n    \"\"\"\n    try:\n        with open(file_path, 'r') as file:\n            text = file.read()\n\n        # Filter out non-ASCII printable characters (ASCII range: 32 to 126 inclusive)\n        filtered_text = ''.join(char for char in text if 32 <= ord(char) <= 126)\n\n        with open(file_path, 'w') as file:\n            file.write(filtered_text)\n\n        return True\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n\n# Example usage:\nresult = remove_non_ascii_printable(filename)\nif result:\n    print(\"Non-ASCII printable characters have been removed.\")\nelse:\n    print(\"An error occurred during processing.\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-10T17:27:47.908420Z","iopub.execute_input":"2023-11-10T17:27:47.909195Z","iopub.status.idle":"2023-11-10T17:27:49.622100Z","shell.execute_reply.started":"2023-11-10T17:27:47.909163Z","shell.execute_reply":"2023-11-10T17:27:49.621057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(filename, encoding='utf-8') as f:\n    text = f.read().strip()","metadata":{"execution":{"iopub.status.busy":"2023-11-10T17:30:47.934982Z","iopub.execute_input":"2023-11-10T17:30:47.935342Z","iopub.status.idle":"2023-11-10T17:30:47.949426Z","shell.execute_reply.started":"2023-11-10T17:30:47.935315Z","shell.execute_reply":"2023-11-10T17:30:47.948195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.zeros((len(text) - seq_length, seq_length, total_chars), dtype=bool)\ny = np.zeros((len(text) - seq_length, total_chars), dtype=bool)\n\nfor i in range(len(text) - seq_length):\n    for t, char in enumerate(text[i:i + seq_length]):\n        X[i, t, char_indices[char]] = 1\n    y[i, char_indices[text[i + seq_length]]] = 1\n","metadata":{"execution":{"iopub.status.busy":"2023-11-10T17:30:49.331189Z","iopub.execute_input":"2023-11-10T17:30:49.332058Z","iopub.status.idle":"2023-11-10T17:32:23.491887Z","shell.execute_reply.started":"2023-11-10T17:30:49.332014Z","shell.execute_reply":"2023-11-10T17:32:23.490760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Assuming 'X' and 'y' are already defined as per your provided code\n\n# Define the split size for training and validation sets\ntrain_size = 0.8  # 80% for training\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size=train_size, random_state=42)\n\n# Now, X_train and y_train are your training set\n# X_val and y_val are your validation set\n","metadata":{"execution":{"iopub.status.busy":"2023-11-10T17:32:32.710507Z","iopub.execute_input":"2023-11-10T17:32:32.711194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sample(preds, temperature=1.0):\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)\n\ndef generate_text(length, diversity):\n    start_index = np.random.randint(0, len(text) - seq_length - 1)\n    sentence = text[start_index: start_index + seq_length]\n    generated = sentence\n    for i in range(length):\n        x_pred = np.zeros((1, seq_length, total_chars))\n        for t, char in enumerate(sentence):\n            x_pred[0, t, char_indices[char]] = 1.\n        preds = model.predict(x_pred, verbose=0)[0]\n        next_index = sample(preds, diversity)\n        next_char = indices_char[next_index]\n        sentence = sentence[1:] + next_char\n        generated += next_char\n    return generated\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if load_saved_model:\n    model = load_model('/content/drive/MyDrive/model-saveLSTM.h5')\nelse:\n    model = Sequential()\n    model.add(LSTM(256, input_shape=(seq_length, total_chars), return_sequences=True))\n    model.add(LSTM(512, return_sequences=True))\n    model.add(LSTM(512))\n    model.add(Dropout(0.2))\n    model.add(Dense(512, activation='relu'))\n    model.add(Dense(total_chars, activation='softmax'))\n    optimizer = Adam(learning_rate=0.002)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n# Early Stopping\nearly_stopping = EarlyStopping(\n    monitor='val_loss',  # Monitor validation loss\n    patience=5,          # Number of epochs with no improvement after which training will be stopped\n    verbose=1,\n    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored quantity.\n)\n\n# Learning Rate Scheduling\nlr_scheduler = ReduceLROnPlateau(\n    monitor='val_loss',  # Monitor validation loss\n    factor=0.97,          # Factor by which the learning rate will be reduced. new_lr = lr * factor\n    patience=3,          # Number of epochs with no improvement after which learning rate will be reduced.\n    verbose=1,\n    min_lr=0.0001        # Lower bound on the learning rate.\n)\n\n# Model training\nhistory = model.fit(\n    X, y,\n    epochs=50,  # Total number of epochs\n    batch_size=32,\n    validation_data=(X_val, y_val),\n    callbacks=[early_stopping, lr_scheduler]  # Add callbacks to training\n)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if train_model:\n    history = model.fit(X, y, batch_size=32, epochs=35)\n    \n    model_save_path = '/kaggle/working/universal-ascii-model.h5'\n    model.save(model_save_path)\n    print(f\"Model saved to {model_save_path}\")\n\n    plt.plot(history.history['loss'])\n    plt.title('Model Loss Over Epochs')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    loss_plot_path = '/kaggle/working/loss.png'\n    plt.savefig(loss_plot_path)\n    print(f\"Loss plot saved to {loss_plot_path}\")\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generated_text = generate_text(200, 4)\nprint(generated_text)\n\nwith open('LSTM-output.txt', 'w') as file:\n    file.write(generate_text(1000, 0.5))\n","metadata":{},"execution_count":null,"outputs":[]}]}